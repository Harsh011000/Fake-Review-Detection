{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ff070a6-7cd2-41d2-947d-74a63b6663e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['did', 'someone', 'say', ',', '``', 'oriental', 'for', '$', '60', \"''\", '?', 'it', 'is', 'a', 'great', 'product', 'for', 'the']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\panwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\panwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the tokenizer if you haven't already\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"Did someone say, \\\"Oriental for $60\\\"?  It is a great product for the\".lower()\n",
    "\n",
    "# Tokenizing the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "540bdcd5-a1d8-45d2-97e4-0ad79786231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\panwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\panwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset saved as 'tokenized_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download NLTK tokenizer if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Data/fake reviews dataset.csv\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_text(text):\n",
    "    if isinstance(text, str):  # Ensure text is a string\n",
    "        tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "        return tokens\n",
    "    return []  # Return empty list if text is NaN or not a string\n",
    "\n",
    "# Apply tokenization to the 'text_' column\n",
    "df['tokenized_text'] = df['text_'].apply(tokenize_text)\n",
    "\n",
    "# Save only the tokenized text and label column\n",
    "df[['tokenized_text', 'label']].to_csv(\"Data/tokenized_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Tokenized dataset saved as 'tokenized_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ba2ebf-6a95-48a6-a805-b3f3162fcf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum tokenized sentence length: 510\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tokenized dataset\n",
    "df = pd.read_csv(\"Data/tokenized_dataset.csv\")\n",
    "\n",
    "# Convert string representation of list back to actual list\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(eval)\n",
    "\n",
    "# Find the maximum length of tokenized sentences\n",
    "max_length = df['tokenized_text'].apply(len).max()\n",
    "\n",
    "print(f\"Maximum tokenized sentence length: {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2c310f-4cb6-47f4-80e5-5bbd676bd626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final padded dataset saved as 'final_padded_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the tokenized dataset\n",
    "df = pd.read_csv(\"Data/tokenized_dataset.csv\")\n",
    "\n",
    "# Convert string representation of list back to an actual list\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(eval)\n",
    "\n",
    "# Load your word_to_index dictionary\n",
    "# Example: word_to_index = {\"love\": 1, \"this\": 2, \"well\": 3, ...}\n",
    "from joblib import load\n",
    "\n",
    "word_to_index = load('Embeddings/word_to_index.joblib')\n",
    "\n",
    "# Function to convert words to indices and pad/truncate sequences\n",
    "def convert_and_pad(tokens, max_len=510, pad_value=0, unk_index=0):\n",
    "    # Convert words to indices using the dictionary, use unk_index if word not found\n",
    "    indexed_tokens=[]\n",
    "    for word in tokens:\n",
    "        if word in word_to_index.keys():\n",
    "            idx=word_to_index.get(word)\n",
    "            indexed_tokens.append(idx)\n",
    "    #indexed_tokens = [word_to_index.get(word, unk_index) for word in tokens]\n",
    "    \n",
    "    # Pad or truncate to the required length\n",
    "    if len(indexed_tokens) < max_len:\n",
    "        indexed_tokens += [pad_value] * (max_len - len(indexed_tokens))  # Pad\n",
    "    else:\n",
    "        indexed_tokens = indexed_tokens[:max_len]  # Truncate if too long\n",
    "    \n",
    "    return indexed_tokens\n",
    "\n",
    "# Apply function to all tokenized sentences\n",
    "df['padded_indices'] = df['tokenized_text'].apply(lambda x: convert_and_pad(x))\n",
    "\n",
    "# Save the modified dataset\n",
    "df[['padded_indices', 'label']].to_csv(\"Data/final_padded_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Final padded dataset saved as 'final_padded_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60572d5a-4c29-4bb8-b7d4-a41e7d11d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset saved as 'final_labeled_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load the final padded dataset\n",
    "df = pd.read_csv(\"Data/final_padded_dataset.csv\")\n",
    "\n",
    "# Convert label values\n",
    "df['label'] = df['label'].map({\"CG\": 0, \"OR\": 1})\n",
    "\n",
    "# Save the updated dataset\n",
    "df.to_csv(\"Data/final_labeled_dataset.csv\", index=False)\n",
    "\n",
    "print(\"Updated dataset saved as 'final_labeled_dataset.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
